{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc60466-3b83-4b79-b0d3-f96dcc7db987",
     "showTitle": true,
     "title": "Create Dataframe() manually"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    >>> spark.createDataFrame([('Alice', 1)]).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a list of dictionaries\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    Create a DataFrame from an RDD.\n    \n    >>> rdd = spark.sparkContext.parallelize([('Alice', 1)])\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from Row instances.\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    Create  a DataFrame from an RDD with the schema in DDL formatted string.\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    \n    When the type is unmatched, it throws an exception.\n    \n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12cc767e-d662-4bf2-9174-89b2e5ac46c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1| Tony|\n|  2|Bruce|\n+---+-----+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "data = [(1,'Tony'),(2,'Bruce')]\n",
    "\n",
    "schema = StructType([StructField(name='id',dataType=IntegerType()),\n",
    "            StructField(name='name', dataType=StringType(),)\n",
    "            ])\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb2b619-c4d5-4827-b528-3da9d287aba0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1| Tony|\n|  2|Bruce|\n+---+-----+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [{'id':1,'name':'Tony'},{'id':2,'name':'Bruce'}]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9355484f-bd8a-45a5-9d45-1c4ef6b8b743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StructType in module pyspark.sql.types:\n\nclass StructType(DataType)\n |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |  \n |  Struct type, consisting of a list of :class:`StructField`.\n |  \n |  This is the data type representing a :class:`Row`.\n |  \n |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n |  A contained :class:`StructField` can be accessed by its name or position.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql.types import *\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1[\"f1\"]\n |  StructField('f1', StringType(), True)\n |  >>> struct1[0]\n |  StructField('f1', StringType(), True)\n |  \n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |  ...     StructField(\"f2\", IntegerType(), False)])\n |  >>> struct1 == struct2\n |  False\n |  \n |  The below example demonstrates how to create a DataFrame based on a struct created\n |  using class:`StructType` and class:`StructField`:\n |  \n |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n |  >>> schema = StructType([\n |  ...     StructField(\"name\", StringType()),\n |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n |  ... ])\n |  >>> df = spark.createDataFrame(data=data, schema=schema)\n |  >>> df.printSchema()\n |  root\n |   |-- name: string (nullable = true)\n |   |-- languagesSkills: array (nullable = true)\n |   |    |-- element: string (containsNull = true)\n |  >>> df.show()\n |  +-----+---------------+\n |  | name|languagesSkills|\n |  +-----+---------------+\n |  |Alice|  [Java, Scala]|\n |  |  Bob|[Python, Scala]|\n |  +-----+---------------+\n |  \n |  Method resolution order:\n |      StructType\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n |      Access fields by name or slice.\n |  \n |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n |      Iterate the fields\n |  \n |  __len__(self) -> int\n |      Return the number of fields.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n |      The method accepts either:\n |      \n |          a) A single parameter which is a :class:`StructField` object.\n |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n |             metadata(optional). The data_type parameter may be either a String or a\n |             :class:`DataType` object.\n |      \n |      Parameters\n |      ----------\n |      field : str or :class:`StructField`\n |          Either the name of the field or a :class:`StructField` object\n |      data_type : :class:`DataType`, optional\n |          If present, the DataType of the :class:`StructField` to create\n |      nullable : bool, optional\n |          Whether the field to add should be nullable (default True)\n |      metadata : dict, optional\n |          Any additional metadata (default None)\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |      ...     StructField(\"f2\", StringType(), True, None)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |  \n |  fieldNames(self) -> List[str]\n |      Returns all field names in a list.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.types import StringType, StructField, StructType\n |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct.fieldNames()\n |      ['f1']\n |  \n |  fromInternal(self, obj: Tuple) -> 'Row'\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -> Dict[str, Any]\n |  \n |  needConversion(self) -> bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -> str\n |  \n |  toInternal(self, obj: Tuple) -> Tuple\n |      Converts a Python object into an internal SQL object.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -> bool\n |      Return self==value.\n |  \n |  __hash__(self) -> int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -> bool\n |      Return self!=value.\n |  \n |  json(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from DataType:\n |  \n |  typeName() -> str from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\nHelp on class StructField in module pyspark.sql.types:\n\nclass StructField(DataType)\n |  StructField(name: str, dataType: pyspark.sql.types.DataType, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n |  \n |  A field in :class:`StructType`.\n |  \n |  Parameters\n |  ----------\n |  name : str\n |      name of the field.\n |  dataType : :class:`DataType`\n |      :class:`DataType` of the field.\n |  nullable : bool, optional\n |      whether the field can be null (None) or not.\n |  metadata : dict, optional\n |      a dict from string to simple type that can be toInternald to JSON automatically\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql.types import StringType, StructField\n |  >>> (StructField(\"f1\", StringType(), True)\n |  ...      == StructField(\"f1\", StringType(), True))\n |  True\n |  >>> (StructField(\"f1\", StringType(), True)\n |  ...      == StructField(\"f2\", StringType(), True))\n |  False\n |  \n |  Method resolution order:\n |      StructField\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, name: str, dataType: pyspark.sql.types.DataType, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  fromInternal(self, obj: ~T) -> ~T\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -> Dict[str, Any]\n |  \n |  needConversion(self) -> bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -> str\n |  \n |  toInternal(self, obj: ~T) -> ~T\n |      Converts a Python object into an internal SQL object.\n |  \n |  typeName(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any]) -> 'StructField' from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -> bool\n |      Return self==value.\n |  \n |  __hash__(self) -> int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -> bool\n |      Return self!=value.\n |  \n |  json(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "help(StructType)\n",
    "help(StructField)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a382913-1787-4353-980e-3f4f80508a8c",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - CreateDataFrame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
